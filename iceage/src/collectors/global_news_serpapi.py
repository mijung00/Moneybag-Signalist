# iceage/src/collectors/global_news_serpapi.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import json
import os
import re
from collections import Counter
from datetime import date, datetime, timezone
from pathlib import Path
from typing import Dict, List

import requests
from dotenv import load_dotenv

load_dotenv()

SERP_ENDPOINT = "https://serpapi.com/search"


def translate_en_to_ko(text: str) -> str:
    """
    TODO: ë‚˜ì¤‘ì— ChatGPT API ë¶™ì—¬ì„œ ìžì—°ìŠ¤ëŸ¬ìš´ ë²ˆì—­ìœ¼ë¡œ êµì²´.
    ì§€ê¸ˆì€ êµ¬ì¡°ë§Œ ë§žì¶°ë‘ê³ , ì¼ë‹¨ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    return text or ""


# ----------------------------
# ê´€ì‹¬ë„ ê¸°ë°˜ ëž­í‚¹ í—¬í¼ í•¨ìˆ˜ë“¤
# ----------------------------

_STOPWORDS = {
    "the", "a", "an", "of", "in", "on", "for", "to", "and", "or",
    "stock", "stocks", "market", "markets", "equities", "share", "shares",
    "index", "indexes", "indices", "today", "rise", "rises", "fall", "falls",
    "up", "down", "after", "as", "amid",
}

# ë©”ì´ì € ë§¤ì²´ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
_SOURCE_WEIGHTS: Dict[str, float] = {
    "Bloomberg": 2.0,
    "Reuters": 2.0,
    "CNBC": 2.0,
    "The Wall Street Journal": 2.0,
    "WSJ": 2.0,
    "Financial Times": 2.0,
    "FT.com": 2.0,
    "Yahoo Finance": 1.5,
    "MarketWatch": 1.5,
    "Barron's": 1.5,
}


def _normalize_text(text: str) -> List[str]:
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    tokens = [t for t in text.split() if t and t not in _STOPWORDS]
    return tokens


def _build_title_tokens(articles: List[Dict]) -> List[set]:
    token_sets: List[set] = []
    for art in articles:
        title = (art.get("title_en") or art.get("title") or "").strip()
        tokens = set(_normalize_text(title))
        token_sets.append(tokens)
    return token_sets


def _rank_articles_by_attention(articles: List[Dict]) -> List[Dict]:
    """
    - ì—¬ëŸ¬ ë©”ì´ì € ë§¤ì²´ì—ì„œ ë°˜ë³µ ì–¸ê¸‰ëœ ì´ìŠˆì¼ìˆ˜ë¡ ì ìˆ˜ â†‘
    - ì†ŒìŠ¤ê°€ ìœ ëª…í• ìˆ˜ë¡ ì ìˆ˜ â†‘
    - ê²°êµ­ score ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    """
    if not articles:
        return []

    token_sets = _build_title_tokens(articles)
    n = len(articles)

    # 1) í† í° ë¹ˆë„ë¡œ ëŒ€ëžµì ì¸ "ì´ìŠˆ ì¤‘ì‹¬ í‚¤ì›Œë“œ" íŒŒì•…
    all_tokens: Counter = Counter()
    for ts in token_sets:
        all_tokens.update(ts)

    # ìžì£¼ ë“±ìž¥í•˜ëŠ” í† í°ë§Œ ì´ìŠˆ í‚¤ì›Œë“œë¡œ ê°„ì£¼
    issue_tokens = {tok for tok, cnt in all_tokens.items() if cnt >= 2}

    scores: List[float] = []
    for i, art in enumerate(articles):
        title_tokens = token_sets[i]

        # (1) ì´ìŠˆ í† í°ê³¼ì˜ ê²¹ì¹¨ ì •ë„
        overlap = len(title_tokens & issue_tokens)

        # (2) ë‹¤ë¥¸ ê¸°ì‚¬ë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê¸°ë°˜ "êµ°ì§‘ í¬ê¸°"
        cluster_size = 1
        for j in range(n):
            if i == j:
                continue
            other = token_sets[j]
            if not other or not title_tokens:
                continue
            inter = len(title_tokens & other)
            union = len(title_tokens | other)
            if union == 0:
                continue
            jaccard = inter / union
            if jaccard >= 0.4:
                cluster_size += 1

        # (3) ì†ŒìŠ¤ ê°€ì¤‘ì¹˜
        source_name = (art.get("source") or "").strip()
        weight = 1.0
        for key, w in _SOURCE_WEIGHTS.items():
            if key.lower() in source_name.lower():
                weight = max(weight, w)

        score = weight * (cluster_size + overlap * 0.5)
        art["score"] = float(score)
        scores.append(score)

    # score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬, tie-breaker: ì›ëž˜ ìˆœì„œ ìœ ì§€
    articles_sorted = sorted(
        articles,
        key=lambda x: x.get("score", 0.0),
        reverse=True,
    )
    return articles_sorted


# ----------------------------
# SerpAPI í˜¸ì¶œ + ì €ìž¥
# ----------------------------

def fetch_global_stock_news(ref_date: date) -> List[Dict]:
    # ì—¬ëŸ¬ ì´ë¦„ ì¤‘ í•˜ë‚˜ë¼ë„ ìžˆìœ¼ë©´ ì‚¬ìš©
    api_key = (
        os.getenv("SERPAPI_API_KEY")
        or os.getenv("SERP_API_KEY")
        or os.getenv("SERPAPI_KEY")
    )

    if not api_key:
        # í‚¤ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ í•´ì™¸ ë‰´ìŠ¤ëŠ” ê±´ë„ˆë›°ê³ , íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ëŒê²Œ í•œë‹¤
        print("[WARN] SerpAPI API key not found (.env). ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    # í•´ì™¸ ì£¼ì‹/ì‹œìž¥ ê´€ë ¨ í‚¤ì›Œë“œ (í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— íŠœë‹)
    query = "stock market OR stocks OR equities OR S&P 500 OR Nasdaq OR Dow Jones"

    params = {
        "engine": "google_news",
        "q": query,
        "hl": "en",
        "gl": "us",
        "api_key": api_key,
        "sort_by": "date",
    }

    try:
        res = requests.get(SERP_ENDPOINT, params=params, timeout=20)
        res.raise_for_status()
        data = res.json()
    except requests.exceptions.HTTPError as e:
        print(f"[WARN] ê¸€ë¡œë²Œ ë‰´ìŠ¤ SerpAPI HTTP ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
    except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
        print(f"[WARN] ê¸€ë¡œë²Œ ë‰´ìŠ¤ SerpAPI ë„¤íŠ¸ì›Œí¬/íƒ€ìž„ì•„ì›ƒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
    except Exception as e:
        print(f"[WARN] ê¸€ë¡œë²Œ ë‰´ìŠ¤ SerpAPI ìš”ì²­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜ˆì™¸: {e}")
        return []


    raw_articles: List[Dict] = []
    for item in data.get("news_results", []):
        title = item.get("title", "")
        snippet = item.get("snippet", "")
        link = item.get("link", "")
        source_name = ""
        src = item.get("source")
        if isinstance(src, dict):
            source_name = src.get("name", "")

        title_ko = translate_en_to_ko(title)
        summary_ko = translate_en_to_ko(snippet or title)

        raw_articles.append(
            {
                "title_en": title,
                "summary_en": snippet,
                "title_ko": title_ko,
                "summary_ko": summary_ko,
                "source": source_name,
                "link": link,
                "published_at": item.get("date", ""),
                "fetched_at": datetime.now(timezone.utc).isoformat(),
            }
        )

    # ðŸ”¹ ê´€ì‹¬ë„ ê¸°ë°˜ìœ¼ë¡œ ëž­í‚¹ ìž¬ì •ë ¬
    ranked = _rank_articles_by_attention(raw_articles)
    return ranked


def save_global_news(ref_date: date) -> Path:
    articles = fetch_global_stock_news(ref_date)

    out_dir = Path("iceage") / "data" / "processed"
    out_dir.mkdir(parents=True, exist_ok=True)
    path = out_dir / f"global_news_{ref_date.isoformat()}.jsonl"

    with path.open("w", encoding="utf-8") as f:
        for art in articles:
            f.write(json.dumps(art, ensure_ascii=False) + "\n")

    print(f"âœ… í•´ì™¸ ë‰´ìŠ¤ ì €ìž¥ ì™„ë£Œ: {path}")
    return path


if __name__ == "__main__":
    import sys

    if len(sys.argv) >= 2:
        ref = date.fromisoformat(sys.argv[1])
    else:
        ref = date.today()

    save_global_news(ref)
