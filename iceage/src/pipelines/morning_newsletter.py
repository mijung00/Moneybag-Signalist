# iceage/src/pipelines/morning_newsletter.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import json
import sys
import re  # [í•„ìˆ˜] ì •ê·œì‹ ëª¨ë“ˆ ìœ ì§€
import logging
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
import pandas as pd
from pathlib import Path
import csv
from typing import List
from datetime import date as _date, timedelta, datetime
from textwrap import dedent

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).resolve().parents[3]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from iceage.src.llm.openai_driver import generate_newsletter_bundle
from iceage.src.analyzers.signalist_history_analyzer import build_signalist_history_markdown

try:
    from iceage.src.pipelines.final_strategy_selector import StrategySelector
except ImportError:
    pass

from iceage.src.data_sources.signalist_today import SignalRow
from iceage.src.signals.signal_volume_pattern import detect_signals_from_volume_anomaly_v2
from iceage.src.data_sources.market_themes import get_market_themes, MarketThemeSummary
from iceage.src.data_sources.sector_themes import get_sector_themes, SectorThemeSummary
from iceage.src.data_sources.investor_flow import load_investor_flow
from iceage.src.data_sources.kr_prices import load_normalized_prices
from iceage.src.data_sources.market_snapshot import get_market_overview
from iceage.src.utils.trading_days import (
    TradingCalendar,
    CalendarConfig,
    compute_reference_date,
    may_run_today,
)
from common.s3_manager import S3Manager


# LLM ìºì‹œ
_LLM_BUNDLE_CACHE: dict[str, dict] = {}

def _get_newsletter_env_suffix() -> str:
    env = os.getenv("NEWSLETTER_ENV", "prod").strip().lower()
    if env in ("", "prod"):
        return ""
    return f"-{env}"

# 1. LLMì—ê²Œ ë³´ë‚¼ ì¬ë£Œë¥¼ í’ì„±í•˜ê²Œ ë§Œë“œëŠ” í•¨ìˆ˜
def _build_llm_payload(ref_date: str) -> dict:
    """LLMì—ê²Œ ë³´ë‚¼ ì¬ë£Œ ì¤€ë¹„ (ì „ëµ ì˜ë„ í¬í•¨)"""
    ref = _date.fromisoformat(ref_date)
    
    # (1) ì‹œì¥ ìš”ì•½
    try: snap = get_market_overview(ref)
    except: snap = {}
    
    headline_bits = []
    indices = snap.get("indices", {})
    if "KOSPI" in indices: headline_bits.append(f"ì½”ìŠ¤í”¼ {indices['KOSPI'][1]:+.2f}%")
    if "KOSDAQ" in indices: headline_bits.append(f"ì½”ìŠ¤ë‹¥ {indices['KOSDAQ'][1]:+.2f}%")
    if "S&P 500" in indices: headline_bits.append(f"S&P500 {indices['S&P 500'][1]:+.2f}%")
    index_summary = " Â· ".join(headline_bits)

    # (2) ì‹œê·¸ë„ ì¢…ëª©
    signal_items = []
    try:
        selector = StrategySelector(ref_date)
        results = selector.select_targets()
        
        candidates = []
        for r in results.get('panic_buying', []):
            r['_strat_hint'] = "íˆ¬ë§¤ê°€ ê³¼ë„í•˜ì—¬ ê¸°ìˆ ì  ë°˜ë“±ì´ ê¸°ëŒ€ë˜ëŠ” êµ¬ê°„"
            candidates.append(r)
        for r in results.get('fallen_angel', []):
            r['_strat_hint'] = "ë‚™í­ ê³¼ëŒ€ ìš°ëŸ‰ì£¼ì˜ ì €ì  ë§¤ìˆ˜ ê¸°íšŒ"
            candidates.append(r)
        for r in results.get('kings_shadow', []):
            r['_strat_hint'] = "ëŒ€í˜•ì£¼ ìƒìŠ¹ ì¶”ì„¸ ì¤‘ ë§¤ë¬¼ ì†Œí™” ê³¼ì • (ëˆŒë¦¼ëª©)"
            candidates.append(r)
        for r in results.get('overheat_short', []):
            r['_strat_hint'] = "ë‹¨ê¸° í­ë“±ìœ¼ë¡œ ì¸í•œ í”¼ë¡œê° ëˆ„ì , ì°¨ìµ ì‹¤í˜„ ë§¤ë¬¼ ì£¼ì˜ (ê³ ì  ì§•í›„)"
            candidates.append(r)
                         
        candidates.sort(key=lambda x: abs(float(x.get('tv_z', 0))), reverse=True)
        top_rows = candidates[:5]
        
        event_map = _get_internal_events(ref_date)
        
        for r in top_rows:
            name = r.get('name', '')
            item = {
                "name": name,
                "change_rate": f"{float(r.get('chg', 0)):+.2f}%",
                "volume_z": f"{float(r.get('tv_z', 0)):.1f}ë°°",
                "strategy_intent": r.get('_strat_hint', ''),
                "keywords": event_map.get(name, ""),
                "is_bull": "ë§¤ìˆ˜" in r.get('_sentiment', 'ë§¤ìˆ˜')
            }
            signal_items.append(item)
            
    except Exception as e:
        print(f"[WARN] LLM Payload ìƒì„± ì˜¤ë¥˜: {e}")

    global_items = load_global_news(ref_date, limit=3)
    articles = [{"title": i.get("title_en",""), "snippet": i.get("summary_en","")} for i in global_items]

    return {
        "ref_date": ref_date,
        "index_summary_line": index_summary,
        "signals": signal_items,
        "global_news": articles,
    }

# 2. í…Œë§ˆ ì„¹ì…˜
def section_themes(ref_date: str) -> str:
    ref = _date.fromisoformat(ref_date)
    themes = get_sector_themes(ref)
    if not themes: return "## Todayâ€™s Market Themes\n\nì˜¤ëŠ˜ì€ ì„¹í„° ê¸°ì¤€ìœ¼ë¡œ ë‘ë“œëŸ¬ì§„ í…Œë§ˆ ì›€ì§ì„ì´ í¬ê²Œ ê´€ì°°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    total_turnover = sum(max(getattr(t, "turnover_sum", 0.0), 0.0) for t in themes)
    
    lines = ["## Todayâ€™s Market Themes", f"ê¸°ì¤€ì¼: {ref_date}", ""]
    
    for t in themes[:3]:
        lines.append(f"### {t.sector}")
        lines.append(f"- ì„¹í„° í‰ê·  ìˆ˜ìµë¥ : **{t.avg_return:+.2f}%**")
        
        if total_turnover > 0:
            share = max(getattr(t, "turnover_sum", 0.0), 0.0) / total_turnover * 100
            flame_count = min(5, int(share // 10))
            if flame_count == 0 and share > 1.0: flame_count = 1
            flames = "ğŸ”¥" * flame_count
            
            if share > 30: comment = "ëˆì´ ì´ ì„¹í„°ì— ìŸì•„ì¡ŒìŠµë‹ˆë‹¤."
            elif share > 20: comment = "ì£¼ë„ ì„¹í„°ì˜ ëª¨ìŠµì„ ë³´ì˜€ìŠµë‹ˆë‹¤."
            elif share > 10: comment = "ìœ ì˜ë¯¸í•œ ìê¸ˆì´ ìœ ì…ë˜ì—ˆìŠµë‹ˆë‹¤."
            else: comment = "íŠ¹ì • ì¢…ëª© ìœ„ì£¼ë¡œ ì›€ì§ì˜€ìŠµë‹ˆë‹¤."
            
            lines.append(f"- ğŸ’° ìˆ˜ê¸‰ ì§‘ì¤‘ë„: {flames} **({share:.1f}%)** - _{comment}_")
        
        lines.append(f"- ëŒ€í‘œ ì¢…ëª©: {', '.join(t.top_stocks)}")
        lines.append("")
        
    return "\n".join(lines)

def _ensure_llm_bundle(ref_date: str) -> dict:
    if ref_date in _LLM_BUNDLE_CACHE:
        return _LLM_BUNDLE_CACHE[ref_date]

    payload = _build_llm_payload(ref_date)
    try:
        bundle = generate_newsletter_bundle(payload)
    except Exception as e:
        print("[WARN] LLM ë²ˆë“¤ ìƒì„± ì‹¤íŒ¨:", repr(e))
        bundle = {}

    _LLM_BUNDLE_CACHE[ref_date] = bundle
    return bundle

ENABLE_INVESTOR_FLOW_SECTION = False 

def section_header_intro(ref_date: str) -> str:
    bundle = _ensure_llm_bundle(ref_date)
    title = bundle.get("title") or f"The Signalist Daily â€” {ref_date}"
    kicker = bundle.get("kicker") or ""
    market_summary = bundle.get("market_one_liner") or ""
    
    ref = _date.fromisoformat(ref_date)
    try:
        snap = get_market_overview(ref)
    except Exception as e:
        print(f"ğŸš¨ [ERROR] get_market_overview failed in section_header_intro: {e}")
        snap = {}
    
    indices = snap.get("indices", {})
    fx = snap.get("fx", {})
    commodities = snap.get("commodities", {})
    crypto = snap.get("crypto", {})

    def _fmt(key, label=None):
        if key in indices: val, pct = indices[key]
        elif key in fx: val, pct = fx[key]
        elif key in commodities: val, pct = commodities[key]
        elif key in crypto: val, pct = crypto[key]
        elif "WTI" in key and "WTI" in commodities: val, pct = commodities["WTI"]
        elif "WTI" in key and "WTI Crude" in commodities: val, pct = commodities["WTI Crude"]
        elif "BTC" in key and "BTC/USD" in crypto: val, pct = crypto["BTC/USD"]
        else: return ""
        
        icon = "ğŸ”º" if pct > 0 else ("ğŸ”¹" if pct < 0 else "-")
        lbl = label if label else key
        return f"{lbl} {val:,.2f} ({icon} {pct:+.2f}%)"

    line_kr = []
    for k, l in [("KOSPI", "ì½”ìŠ¤í”¼"), ("KOSDAQ", "ì½”ìŠ¤ë‹¥"), ("USD/KRW", "ì›/ë‹¬ëŸ¬")]:
        r = _fmt(k, l)
        if r: line_kr.append(r)
        
    line_us = []
    for k, l in [("Dow Jones", "ë‹¤ìš°"), ("NASDAQ", "ë‚˜ìŠ¤ë‹¥"), ("S&P 500", "S&P500")]:
        r = _fmt(k, l)
        if r: line_us.append(r)
        
    line_macro = []
    for k, l in [("WTI", "WTIìœ "), ("BTC", "ë¹„íŠ¸ì½”ì¸")]:
        r = _fmt(k, l)
        if r: line_macro.append(r)

    lines = [f"# {title}", ""]
    if kicker:
        lines.append(f"_{kicker}_")
        lines.append("")
    
    lines.append("## ì˜¤ëŠ˜ì˜ ì‹œì¥ í•œëˆˆì— ë³´ê¸°")
    lines.append(f"ê¸°ì¤€ì¼: {ref_date}")
    lines.append("")
    
    if market_summary:
        lines.append(market_summary)
        lines.append("")
    
    if line_kr: lines.append(f"**í•œêµ­**: " + " â”‚ ".join(line_kr)); lines.append("")
    if line_us: lines.append(f"**ë¯¸êµ­**: " + " â”‚ ".join(line_us)); lines.append("")
    if line_macro: lines.append(f"**ê¸°íƒ€**: " + " â”‚ ".join(line_macro)); lines.append("")

    return "\n".join(lines)

def _select_signalist_today_rows(ref: _date) -> List[SignalRow]:
    try: all_rows = detect_signals_from_volume_anomaly_v2(ref)
    except Exception: all_rows = []
    if not all_rows: return []

    def _vol(r):
        try: return abs(float(getattr(r, "vol_sigma", 0.0)))
        except Exception: return 0.0
    candidates = sorted(all_rows, key=_vol, reverse=True)

    def _sector(r):
        val = getattr(r, "sector", "") or getattr(r, "theme", "")
        return str(val).strip()

    pos_rows = [r for r in candidates if getattr(r, "vol_sigma", 0.0) > 0]
    neg_rows = [r for r in candidates if getattr(r, "vol_sigma", 0.0) < 0]
    TOP_N = 5; PER_SECTOR_LIMIT = 2
    selected: list = []; seen: set[tuple] = set(); sector_counts: dict[str, int] = {}

    def _can_add(r) -> bool:
        k = (r.name, getattr(r, "vol_sigma", 0.0))
        if k in seen: return False
        sec = _sector(r)
        if not sec: return True
        if sector_counts.get(sec, 0) >= PER_SECTOR_LIMIT: return False
        return True

    def _add(r):
        if not _can_add(r): return
        k = (r.name, getattr(r, "vol_sigma", 0.0))
        seen.add(k); selected.append(r)
        sec = _sector(r)
        if sec: sector_counts[sec] = sector_counts.get(sec, 0) + 1

    if pos_rows: _add(pos_rows[0])
    if neg_rows: _add(neg_rows[0])
    for r in candidates:
        if len(selected) >= TOP_N: break
        _add(r)
    if len(selected) < TOP_N:
        for r in candidates:
            if len(selected) >= TOP_N: break
            k = (r.name, getattr(r, "vol_sigma", 0.0))
            if k in seen: continue
            seen.add(k); selected.append(r)
    return selected

# ---------------------------------------------------------
# [í•µì‹¬] ì´ìŠˆ í‚¤ì›Œë“œ ì¶”ì¶œê¸° V2 (ì •ê·œì‹ ê°•í™” ë²„ì „ ìœ ì§€)
# ---------------------------------------------------------
def _extract_keyword_from_title(title: str, stock_name: str) -> str:
    if not title: return "-"
    
    # 1. ê´„í˜¸ ë° ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±° (ì˜ˆ: [íŠ¹ì§•ì£¼], (ì†ë³´))
    title = re.sub(r"\[.*?\]", " ", title)
    title = re.sub(r"\(.*?\)", " ", title)
    
    # 2. ì¢…ëª©ëª… ì œê±° (ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜)
    title = title.replace(stock_name, " ")
    
    # 3. íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë”°ì˜´í‘œ, ì , ì‰¼í‘œ, ì¤„í‘œ ë“± -> ê³µë°±)
    # í•œê¸€, ì˜ë¬¸, ìˆ«ì ë¹¼ê³  ë‹¤ ì§€ì›€
    title = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", title)
    
    # 4. ë¶ˆìš©ì–´(Stopwords) ì œê±° - ë‰´ìŠ¤ ìƒíˆ¬ì–´
    stop_words = [
        "íŠ¹ì§•ì£¼", "ê¸‰ë“±", "ìƒìŠ¹", "í•˜ë½", "ì•½ì„¸", "ê°•ì„¸", "ì£¼ê°€", "ì „ë§", "ì´ìŠˆ", 
        "ê³µì‹œ", "ì²´ê²°", "ê·œëª¨", "ì¢…ëª©", "ê´€ë ¨ì£¼", "í…Œë§ˆ", "ë¶„ì„", "ì†ë³´", "ë‹¨ë…",
        "ì˜í–¥", "ì£¼ëª©", "ìµœê³ ", "ìµœì €", "ê²½ì‹ ", "ëŒíŒŒ", "ë§ˆê°", "ì¶œë°œ", "ì˜¤ì „", "ì˜¤í›„",
        "í¬ì°©", "ì²´í¬", "ì£¼ì˜", "ë¹„ìƒ", "ê¸°ëŒ€", "ìš°ë ¤", "ì‡¼í¬", "ì„œí”„ë¼ì´ì¦ˆ", "ì‹¤ì ",
        "ë°œí‘œ", "ê³µê°œ", "ê°œì‹œ", "ì„±ê³µ", "ì²´ê²°", "í™•ì •", "ì§„ì…", "í™•ëŒ€", "ì¶•ì†Œ", "ìƒí•œê°€", "í•˜í•œê°€"
    ]
    for w in stop_words:
        title = title.replace(w, " ")
        
    # 5. ìˆ«ì ì œê±° ë° 1ê¸€ì ì œê±°
    words = title.split()
    cleaned_words = []
    for w in words:
        if re.search(r"\d", w): continue
        if len(w) < 2: continue
        
        # ëì— ë¶™ì€ ì¡°ì‚¬ ì œê±° (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        if len(w) >= 3 and w[-1] in ['ì—', 'ë¡œ', 'ì„', 'ë¥¼', 'ê°€', 'ì´', 'ì€', 'ëŠ”', 'ì˜']:
             w = w[:-1]
        cleaned_words.append(w)
        
    if not cleaned_words: return "-"
        
    # 6. ê°€ì¥ ê¸´ ë‹¨ì–´ ì„ íƒ
    return max(cleaned_words, key=len)

def _get_internal_events(ref_date: str) -> dict[str, str]:
    news_path = PROJECT_ROOT / "iceage" / "data" / "raw" / f"kr_stock_event_news_{ref_date}.jsonl"
    event_map = {}
    if not news_path.exists(): return {}
    
    with news_path.open(encoding="utf-8") as f:
        for line in f:
            try:
                item = json.loads(line)
                name = item.get("stock_name")
                title = item.get("title")
                if name and title:
                    if name not in event_map:
                        keyword = _extract_keyword_from_title(title, name)
                        if keyword and keyword != "-":
                            event_map[name] = keyword
            except: continue
    return event_map

def section_market_thermometer(ref_date: str) -> str:
    ref = _date.fromisoformat(ref_date)
    try:
        snap = get_market_overview(ref)
        indices = snap.get("indices", {})
        changes = []
        if "KOSPI" in indices: changes.append(indices["KOSPI"][1])
        if "KOSDAQ" in indices: changes.append(indices["KOSDAQ"][1])
        if not changes: return ""
        avg_chg = sum(changes) / len(changes)
    except: return ""

    if avg_chg >= 1.5:
        status = "ğŸ”¥ ê³¼ì—´ (Extreme Greed)"; gauge = "[ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥]"; comment = "ì‹œì¥ì´ ëœ¨ê²ìŠµë‹ˆë‹¤. ì¶”ê²© ë§¤ìˆ˜ë³´ë‹¤ëŠ” ì°¨ìµ ì‹¤í˜„ì„ ê³ ë ¤í•  êµ¬ê°„ì…ë‹ˆë‹¤."
    elif avg_chg >= 0.5:
        status = "â˜€ï¸ ë§‘ìŒ (Greed)"; gauge = "[ğŸŸ¥ğŸŸ¥ğŸŸ¥â¬œâ¬œ]"; comment = "íˆ¬ì ì‹¬ë¦¬ê°€ ì‚´ì•„ë‚¬ìŠµë‹ˆë‹¤. ì£¼ë„ì£¼ ìœ„ì£¼ì˜ ì ‘ê·¼ì´ ìœ íš¨í•©ë‹ˆë‹¤."
    elif avg_chg >= -0.5:
        status = "â˜ï¸ íë¦¼ (Neutral)"; gauge = "[â¬œâ¬œğŸŸ©â¬œâ¬œ]"; comment = "ë°©í–¥ì„± íƒìƒ‰ êµ¬ê°„ì…ë‹ˆë‹¤. ê°œë³„ ì¢…ëª© ì´ìŠˆì— ì§‘ì¤‘í•˜ì„¸ìš”."
    elif avg_chg >= -1.5:
        status = "â˜” ë¹„ (Fear)"; gauge = "[ğŸŸ¦ğŸŸ¦ğŸŸ¦â¬œâ¬œ]"; comment = "íˆ¬ì‹¬ì´ ìœ„ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. ë³´ìˆ˜ì ì¸ ê´€ì ì´ í•„ìš”í•©ë‹ˆë‹¤."
    else:
        status = "â„ï¸ í˜¹í•œ (Extreme Fear)"; gauge = "[ğŸŸ¦ğŸŸ¦ğŸŸ¦ğŸŸ¦ğŸŸ¦]"; comment = "ê³µí¬ êµ¬ê°„ì…ë‹ˆë‹¤. íˆ¬ë§¤ ë™ì°¸ë³´ë‹¤ëŠ” 'íŒ¨ë‹‰ ë°”ì‰' ê¸°íšŒë¥¼ ë…¸ë¦¬ì„¸ìš”."

    return dedent(f"""
    ### ğŸŒ¡ï¸ ì˜¤ëŠ˜ì˜ ì‹œì¥ ì˜¨ë„: {status}
    **{gauge}**
    > *"{comment}"*
    """).strip()

def section_signalist_today(ref_date: str) -> str:
    try:
        from iceage.src.pipelines.final_strategy_selector import StrategySelector
        selector = StrategySelector(ref_date)
        results = selector.select_targets()
        
        candidates = []
        for r in results.get('panic_buying', []) + results.get('fallen_angel', []) + results.get('kings_shadow', []):
            r['_sentiment'] = 'ğŸ“ˆ ë§¤ìˆ˜ ìš°ìœ„'
            bucket = r.get('size_bucket', '')
            if bucket == 'large': r['_tone'] = "ğŸ”µ ëŒ€í˜•ì£¼ ìˆ˜ê¸‰"
            elif bucket == 'mid': r['_tone'] = "ğŸŸ¡ ì¤‘í˜•ì£¼ ë°˜ë“±"
            else: r['_tone'] = "ğŸŸ¢ ì†Œí˜•ì£¼ ê¸‰ë“±"
            candidates.append(r)
            
        shorts = results.get('overheat_short', [])
        if shorts:
            shorts = sorted(shorts, key=lambda x: abs(float(x.get('tv_z', 0))), reverse=True)[:1]
            for r in shorts:
                r['_sentiment'] = 'ğŸ“‰ ë§¤ë„ ìš°ìœ„'
                r['_tone'] = "ğŸš¨ ê³¼ì—´ ê²½ë³´"
                candidates.append(r)
            
        candidates.sort(key=lambda x: abs(float(x.get('tv_z', 0))), reverse=True)
        rows = candidates[:5]
        
    except Exception:
        rows = []

    if not rows:
        return "## ì˜¤ëŠ˜ì˜ ë ˆì´ë” í¬ì°© (The Signalist Radar)\n\ní¬ì°©ëœ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤."

    event_map = _get_internal_events(ref_date)

    intro = dedent("""
    ## ì˜¤ëŠ˜ì˜ ë ˆì´ë” í¬ì°© (The Signalist Radar)
    **"ë°ì´í„°ê°€ ë°œê²¬í•œ ìˆ˜ê¸‰ì˜ ë³€ê³¡ì "**
    Signalistë ˆì´ë”ëŠ” ì‹œì´ë³„ íŠ¹ì„±ê³¼ ê±°ë˜ëŒ€ê¸ˆ ê´´ë¦¬ìœ¨ì„ ì…ì²´ì  ë¶„ì„í•˜ì—¬, **ìœ ì˜ë¯¸í•œ íë¦„ì´ í¬ì°©ëœ ì¢…ëª©**ì„ ì„ ë³„í•©ë‹ˆë‹¤.
    ë‹¨ìˆœí•œ ê°€ê²© ë“±ë½ì´ ì•„ë‹Œ, **í‰ì†Œ ëŒ€ë¹„ ë¹„ì •ìƒì ì¸ ê±°ë˜ ê°•ë„**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥ì„±ì´ ë†’ì€ êµ¬ê°„ì„ íƒì§€í–ˆìŠµë‹ˆë‹¤.
    """).strip()

    header = "| ì¢…ëª©ëª… | ì¢…ê°€ | ë“±ë½ (í­) | ê´´ë¦¬ìœ¨ | ìˆ˜ê¸‰ ë°©í–¥ | ì´ìŠˆ í‚¤ì›Œë“œ |"
    sep = "|---|---|---|---|---|---|"
    
    body = []
    for r in rows:
        name = r.get('name', '')
        close_val = int(r.get('close', 0))
        close_str = f"{close_val:,}"
        chg_pct = float(r.get('chg', 0))
        prev_close = close_val / (1 + chg_pct/100)
        chg_won = int(close_val - prev_close)
        
        if chg_pct > 0: chg_str = f"**+{chg_pct:.2f}%**<br><small>(â–²{chg_won:,})</small>"
        elif chg_pct < 0: chg_str = f"{chg_pct:.2f}%<br><small>(â–¼{abs(chg_won):,})</small>"
        else: chg_str = "0.00%"
            
        sigma = f"{float(r.get('tv_z', 0)):+.1f}Ïƒ"
        display_tone = f"{r.get('_sentiment', '-')}<br>({r.get('_tone', '')})"
        event_key = event_map.get(name, "-")

        body.append(f"| {name} | {close_str} | {chg_str} | {sigma} | {display_tone} | {event_key} |")

    table = "\n".join([header, sep] + body)
    
    memo_lines = ["\n### ğŸ§ ì¢…ëª©ë³„ ê´€ì°° ë©”ëª¨"]
    bundle = _ensure_llm_bundle(ref_date)
    sig_comments = bundle.get("signal_comments") or {}
    
    for r in rows:
        name = r.get('name')
        comment = sig_comments.get(name) or "íŠ¹ì´ ìˆ˜ê¸‰ í¬ì°©"
        memo_lines.append(f"- **{name}**: {comment}")

    memo_md = "\n".join(memo_lines)
    
    return f"{intro}\n\nê¸°ì¤€ì¼: {ref_date}\n\n{table}\n{memo_md}\n\n_ìœ„ ë¦¬ìŠ¤íŠ¸ëŠ” ì•Œê³ ë¦¬ì¦˜ ì¶”ì¶œ ê²°ê³¼ì´ë©°, íˆ¬ì ê¶Œìœ ê°€ ì•„ë‹™ë‹ˆë‹¤._"

def section_signalist_history(ref_date: str, window_days: int = 90) -> str:
    ref = _date.fromisoformat(ref_date)
    return build_signalist_history_markdown(ref, lookback_days=window_days)

def load_kr_news_cleaned(ref_date: str, limit: int = 5) -> list[dict]:
    path = Path("iceage") / "data" / "processed" / f"kr_news_cleaned_{ref_date}.jsonl"
    if not path.exists(): return []
    items = []
    with path.open(encoding="utf-8") as f:
        for line in f:
            try: items.append(json.loads(line))
            except: continue
            if len(items) >= limit: break
    return items

def load_global_news(ref_date: str, limit: int = 3) -> list[dict]:
    path = Path("iceage") / "data" / "processed" / f"global_news_{ref_date}.jsonl"
    if not path.exists(): return []
    items = []
    with path.open(encoding="utf-8") as f:
        for line in f:
            try: items.append(json.loads(line))
            except: continue
            if len(items) >= limit: break
    return items

def section_news_digest(ref_date: str) -> str:
    kr_items = load_kr_news_cleaned(ref_date, limit=5)
    global_items = load_global_news(ref_date, limit=3)
    lines = ["## Todayâ€™s Top News"]

    if kr_items:
        lines.append("\n### êµ­ë‚´ ì£¼ìš” ë‰´ìŠ¤\n")
        for i, item in enumerate(kr_items, 1):
            lines.append(f"{i}. [{item.get('title')}]({item.get('link')}) ({item.get('source')})")
        lines.append("")

    if global_items:
        bundle = _ensure_llm_bundle(ref_date)
        llm_summary = bundle.get("global_summary")
        lines.append("\n### í•´ì™¸ ì£¼ìš” ë‰´ìŠ¤\n")
        if isinstance(llm_summary, dict):
            if llm_summary.get("headline"): lines.append(f"**{llm_summary['headline']}**\n")
            if llm_summary.get("summary"): lines.append(f"{llm_summary['summary']}\n")
            for b in llm_summary.get("bullets", []): lines.append(f"- {b}")
            lines.append("")
        for i, item in enumerate(global_items, 1):
            t = item.get("title_en") or item.get("title")
            lines.append(f"{i}. [{t}]({item.get('link')}) ({item.get('source')})")

    return "\n".join(lines).strip()

def section_global_minute(ref_date: str) -> str:
    ref = _date.fromisoformat(ref_date)
    try:
        snap = get_market_overview(ref)
    except Exception as e:
        print(f"ğŸš¨ [ERROR] get_market_overview failed in section_global_minute: {e}")
        snap = {}
    indices = snap.get("indices", {})
    fx = snap.get("fx", {})
    commodities = snap.get("commodities", {})

    sp_level, sp_pct = indices.get("S&P 500", (None, None))
    ndq_level, ndq_pct = indices.get("NASDAQ", (None, None))
    dxy_level, dxy_pct = fx.get("DXY", (None, None))
    wti_level, wti_pct = commodities.get("WTI", commodities.get("WTI Crude", (None, None)))

    lines = ["## Global Minute", f"ê¸°ì¤€ì¼: {ref_date}", ""]
    lines.append("### US")
    if sp_pct is not None:
        lines.append(f"- ì´ìŠˆ: S&P 500 {sp_level:,.2f} ({sp_pct:+.2f}%), NASDAQ {ndq_pct:+.2f}%")
        if sp_pct > 0.4: impact = "ì„±ì¥ì£¼Â·ê¸°ìˆ ì£¼ ì¤‘ì‹¬ìœ¼ë¡œ ìœ„í—˜ì„ í˜¸ê°€ ê°•í™”ëœ íë¦„ì…ë‹ˆë‹¤."
        elif sp_pct < -0.4: impact = "ê¸ˆë¦¬Â·ì‹¤ì  ë¶€ë‹´ìœ¼ë¡œ ìœ„í—˜ìì‚° íšŒí”¼ ì‹¬ë¦¬ê°€ ë‚˜íƒ€ë‚œ êµ¬ê°„ì…ë‹ˆë‹¤."
        else: impact = "ì‹¤ì Â·ë§¤í¬ë¡œ ì´ë²¤íŠ¸ë¥¼ ì†Œí™”í•˜ë©° ë°©í–¥ì„±ì„ íƒìƒ‰í•˜ëŠ” ì¡°ì • êµ¬ê°„ì…ë‹ˆë‹¤."
        lines.append(f"- í•´ì„: {impact}")
    else: lines.append("- ì´ìŠˆ: ë°ì´í„° ë¶€ì¡±")
    lines.append("")

    lines.append("### ë‹¬ëŸ¬/í™˜ìœ¨")
    if dxy_pct is not None:
        lines.append(f"- ì´ìŠˆ: ë‹¬ëŸ¬ ì¸ë±ìŠ¤(DXY) {dxy_level:,.2f} ({dxy_pct:+.2f}%)")
        if dxy_pct < -0.3: impact = "ë‹¬ëŸ¬ ì•½ì„¸ êµ¬ê°„ìœ¼ë¡œ, ì‹ í¥êµ­ ìì‚°ê³¼ ìœ„í—˜ìì‚°ì— ìƒëŒ€ì ìœ¼ë¡œ ìš°í˜¸ì ì¸ í™˜ê²½ì…ë‹ˆë‹¤."
        elif dxy_pct > 0.3: impact = "ë‹¬ëŸ¬ ê°•ì„¸ë¡œ, ì•ˆì „ìì‚° ì„ í˜¸ ë° ìœ ë™ì„± ê²½ê³„ ì‹¬ë¦¬ê°€ ë°˜ì˜ëœ íë¦„ì…ë‹ˆë‹¤."
        else: impact = "ë‹¬ëŸ¬ê°€ ëšœë ·í•œ ë°©í–¥ì„± ì—†ì´ ë“±ë½í•˜ë©° ë‹¨ê¸° ì´ë²¤íŠ¸ë¥¼ ê´€ë§í•˜ëŠ” êµ¬ê°„ì…ë‹ˆë‹¤."
        lines.append(f"- í•´ì„: {impact}")
    else: lines.append("- ì´ìŠˆ: ë°ì´í„° ë¶€ì¡±")
    lines.append("")

    lines.append("### ì›ìì¬/ì—ë„ˆì§€")
    if wti_pct is not None:
        lines.append(f"- ì´ìŠˆ: WTI {wti_level:,.2f}ë‹¬ëŸ¬ ({wti_pct:+.2f}%)")
        if wti_pct > 0.5: impact = "ìœ ê°€ ìƒìŠ¹ìœ¼ë¡œ ì¸í”Œë ˆì´ì…˜Â·ì›ê°€ ë¶€ë‹´ì— ëŒ€í•œ ê²½ê³„ê°€ ë‹¤ì‹œ ë¶€ê°ë  ìˆ˜ ìˆëŠ” êµ¬ê°„ì…ë‹ˆë‹¤."
        elif wti_pct < -0.5: impact = "ìœ ê°€ í•˜ë½ìœ¼ë¡œ ë¬¼ê°€ ë¶€ë‹´ ì™„í™” ê¸°ëŒ€ê°€ ì»¤ì§€ë©° ìœ„í—˜ìì‚°ì— ìš°í˜¸ì ì¸ í™˜ê²½ì…ë‹ˆë‹¤."
        else: impact = "ìœ ê°€ê°€ ë°•ìŠ¤ê¶Œ ë“±ë½ì„ ì´ì–´ê°€ë©° ê³µê¸‰Â·ìˆ˜ìš” ì´ìŠˆë¥¼ ì†Œí™”í•˜ëŠ” êµ¬ê°„ì…ë‹ˆë‹¤."
        lines.append(f"- í•´ì„: {impact}")
    else: lines.append("- ì´ìŠˆ: ë°ì´í„° ë¶€ì¡±")
    lines.append("")

    return "\n".join(lines)

def section_investors_mind(topic: str, body: str) -> str:
    if not topic or not body: return ""
    return dedent(f"""
    ## ğŸ§˜ Investor's Mind: {topic}
    {body}
    """).strip()

def _find_col(columns, candidates):
    cols = list(columns)
    for c in candidates:
        if c in columns: return c
    for c in candidates:
        for col in cols:
            if c in str(col): return col
    return None

def _load_turnover_by_market(ref: _date) -> dict[str, float]:
    """
    [ìˆ˜ì •] ê±°ë˜ëŒ€ê¸ˆ ì»¬ëŸ¼ì´ ìˆì–´ë„ ë°ì´í„°ê°€ 0ì´ë©´, 
    ì¢…ê°€*ê±°ë˜ëŸ‰ìœ¼ë¡œ ê°•ì œ ë³µêµ¬í•˜ì—¬ ë°˜í™˜í•˜ëŠ” ì•ˆì „ì¥ì¹˜ ì¶”ê°€
    """
    raw_path = Path("iceage") / "data" / "raw" / f"kr_prices_{ref.isoformat()}.csv"
    if not raw_path.exists(): return {}
    
    try: 
        df = pd.read_csv(raw_path)
    except: return {}
    
    # ìˆ«ì ë³€í™˜ í—¬í¼
    def _clean(x):
        try: return float(str(x).replace(",", ""))
        except: return 0.0
        
    cols = set(df.columns)
    
    # 1. ì‹œì¥ êµ¬ë¶„ ì»¬ëŸ¼ ì°¾ê¸°
    market_col = _find_col(cols, ["market", "ì‹œì¥êµ¬ë¶„", "ì‹œì¥", "Market"])
    if not market_col: return {}
    
    # 2. ê±°ë˜ëŒ€ê¸ˆ ìš°ì„  ì‹œë„
    value_col = _find_col(cols, ["trading_value", "ê±°ë˜ëŒ€ê¸ˆ"])
    if value_col:
        df["_turnover_"] = df[value_col].apply(_clean)
    else:
        df["_turnover_"] = 0.0
        
    # 3. [í•µì‹¬] ê±°ë˜ëŒ€ê¸ˆì´ ë¹„ì–´ìˆê±°ë‚˜ í•©ê³„ê°€ 0ì´ë©´ ê°•ì œ ê³„ì‚° (ì‹¬íì†Œìƒìˆ )
    if df["_turnover_"].sum() == 0:
        close_col = _find_col(cols, ["close", "ì¢…ê°€", "í˜„ì¬ê°€"])
        vol_col = _find_col(cols, ["volume", "ê±°ë˜ëŸ‰"])
        
        if close_col and vol_col:
            df["_turnover_"] = df[close_col].apply(_clean) * df[vol_col].apply(_clean)

    return df.groupby(market_col)["_turnover_"].sum().to_dict()

def section_numbers_that_matter(ref_date: str) -> str:
    ref = _date.fromisoformat(ref_date)
    lines = ["## Numbers that Matter", f"ê¸°ì¤€ì¼: {ref_date}", ""]
    
    by_market = _load_turnover_by_market(ref)
    if by_market:
        lines.append("### ì˜¤ëŠ˜ êµ­ë‚´ ì£¼ì‹ ê±°ë˜ëŒ€ê¸ˆ (ì¡°ì› ë‹¨ìœ„, ì¶”ì •)")
        total = 0.0
        for market_name, v in by_market.items():
            total += float(v)
            trillions = float(v) / 1_000_000_000_000
            lines.append(f"- {market_name}: {trillions:,.1f}ì¡°")
        lines.append(f"- í•©ê³„: {total / 1_000_000_000_000:,.1f}ì¡°")
        lines.append("")
        
    fx_series = []
    for back in range(4, -1, -1):
        d = ref - timedelta(days=back)
        try:
            snap = get_market_overview(d)
            fx = snap.get("fx", {})
            if "USD/KRW" in fx: fx_series.append((d, fx["USD/KRW"][0]))
        except: continue
        
    if fx_series:
        lines.append("### USD/KRW í™˜ìœ¨ (ìµœê·¼ ì¼ì)")
        prev = None
        for d, level in fx_series:
            diff = f"({level-prev:+.2f})" if prev else ""
            lines.append(f"- {d.isoformat()}: {level:,.2f} {diff}")
            prev = level
        lines.append("")
        
    if ENABLE_INVESTOR_FLOW_SECTION:
        flow_map = load_investor_flow(ref)
        if flow_map:
            lines.append("### íˆ¬ììë³„ ë§¤ë§¤ ë™í–¥ (ë‹¨ìœ„: ì–µì›, ìˆœë§¤ìˆ˜ ê¸°ì¤€)")
            lines.append("| ì‹œì¥ | ê°œì¸ | ì™¸êµ­ì¸ | ê¸°ê´€ |")
            lines.append("|------|------|--------|------|")
            for m in ["KOSPI", "KOSDAQ"]:
                s = flow_map.get(m)
                if s:
                    p = s.net_by_investor.get("ê°œì¸", 0)
                    f = s.net_by_investor.get("ì™¸êµ­ì¸", 0)
                    i = s.net_by_investor.get("ê¸°ê´€", 0)
                    lines.append(f"| {m} | {p:,.1f} | {f:,.1f} | {i:,.1f} |")
            lines.append("")
            
    return "\n".join(lines)

def extract_first_sentence(text: str) -> str:
    if not text: return ""
    cleaned = " ".join(text.split())
    sentences = re.split(r'(?<=[\.!?])\s+', cleaned)
    return sentences[0].strip() if sentences else cleaned.strip()

def section_morning_quote(quote: str) -> str:
    return dedent(f"""
    ## Morning Quote
    > {quote}
    """).strip()

def section_footer() -> str:
    return dedent(f"""
    ---
    ë³¸ ì½˜í…ì¸ ëŠ” íˆ¬ì ê¶Œìœ  ëª©ì ì´ ì•„ë‹Œ ì •ë³´ ì œê³µìš©ì…ë‹ˆë‹¤.  
    The Signalist Â© 2025 All Rights Reserved.  [êµ¬ë…í•´ì§€]  [ì˜ê²¬ë³´ë‚´ê¸°]
    """).strip()

MIND_TOPICS = ["í™•ì‹ ë³´ë‹¤ ìœ ì—°í•¨", "ì†ì‹¤ì„ ëŒ€í•˜ëŠ” íƒœë„", "ê³¼ì‰ í™•ì‹ ì˜ í•¨ì •", "ë³µë¦¬ì™€ ê¸°ë‹¤ë¦¼", "í¬ì§€ì…˜ ì‚¬ì´ì§•"]
def pick_topic_and_body(ref_date: str) -> tuple[str, str]:
    import random
    fallback_topic = random.choice(MIND_TOPICS)
    fallback_body = "í‰ì •ì‹¬ì„ ìœ ì§€í•˜ì„¸ìš”. ì‹œì¥ì€ ì–¸ì œë‚˜ ê¸°íšŒë¥¼ ì¤ë‹ˆë‹¤."
    try:
        bundle = _ensure_llm_bundle(ref_date)
        im = bundle.get("investor_mind") or {}
        return im.get("topic", fallback_topic), im.get("body", fallback_body)
    except: return fallback_topic, fallback_body

def render_newsletter(ref_date: str) -> str:
    topic, body = pick_topic_and_body(ref_date)
    parts = [
        section_header_intro(ref_date),
        section_market_thermometer(ref_date),
        section_signalist_today(ref_date),
        section_signalist_history(ref_date),
        section_themes(ref_date),
        # IPO ì„¹ì…˜ ì œê±°
        section_global_minute(ref_date),
        section_news_digest(ref_date),
        section_investors_mind(topic, body),
        section_numbers_that_matter(ref_date),
        section_footer()
    ]
    return "\n\n".join([p for p in parts if p])

def log_signalist_today(ref_date: str, rows: list, force: bool = True) -> None:
    if not rows: return
    out_dir = PROJECT_ROOT / "iceage" / "data" / "processed"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "signalist_today_log.csv"
    
    new_records = []
    for r in rows:
        if hasattr(r, '__dict__'):
            d = {
                "signal_date": ref_date, "code": getattr(r, "code", ""), "name": r.name,
                "close": r.close, "vol_sigma": float(getattr(r, "vol_sigma", 0.0)),
                "sentiment": getattr(r, "sentiment", ""), "insight": getattr(r, "insight", "")
            }
        else:
            d = {
                "signal_date": ref_date, "code": str(r.get('code', '')).zfill(6),
                "name": r.get('name', ''), "close": r.get('close', 0),
                "vol_sigma": float(r.get('tv_z', 0) or r.get('vol_sigma', 0)),
                "sentiment": r.get('_sentiment') or r.get('sentiment', ''),
                "insight": r.get('_insight') or r.get('insight', '')
            }
        new_records.append(d)
        
    new_records.sort(key=lambda x: abs(x['vol_sigma']), reverse=True)
    new_records = new_records[:5]
    df_new = pd.DataFrame(new_records)
    
    if out_path.exists():
        try:
            df_old = pd.read_csv(out_path, encoding="utf-8-sig")
            
            # [Fix] ì»¬ëŸ¼ í˜¸í™˜ì„± ì²´í¬ (ê³¼ê±° ë°±í•„ ë°ì´í„° í˜¸í™˜)
            if "date" in df_old.columns and "signal_date" not in df_old.columns:
                df_old.rename(columns={"date": "signal_date"}, inplace=True)
            
            if "tv_z" in df_old.columns and "vol_sigma" not in df_old.columns:
                df_old.rename(columns={"tv_z": "vol_sigma"}, inplace=True)
                
            if "signal_date" not in df_old.columns:
                print("[ERROR] ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ì— 'signal_date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë®ì–´ì“°ê¸°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë³‘í•©ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return 

            df_old = df_old[df_old["signal_date"] != ref_date]
            df_all = pd.concat([df_old, df_new], ignore_index=True)
        except Exception as e:
            print(f"[ERROR] ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ë³‘í•© ì‹¤íŒ¨: {e}")
            print("   -> ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ ìƒˆ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return # ì•ˆì „ì¥ì¹˜: ì—ëŸ¬ë‚˜ë©´ ê·¸ëƒ¥ ë¦¬í„´ (ë®ì–´ì“°ê¸° ë°©ì§€)
    else: 
        df_all = df_new
    
    df_all = df_all.sort_values("signal_date")
    df_all.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"âœ… [Log Saved] {ref_date} ì‹œê·¸ë„ {len(new_records)}ê°œ ì €ì¥ ì™„ë£Œ!")

def main():
    cal = TradingCalendar(CalendarConfig())
    if len(sys.argv) >= 2: ref_date = sys.argv[1]
    else: 
        ref = compute_reference_date(cal, datetime.now())
        ref_date = ref.isoformat()

    print(f"\nğŸ“… Newsletter ref_date: {ref_date}")

    try:
        selector = StrategySelector(ref_date)
        results = selector.select_targets()
        candidates = []
        for r in results.get('panic_buying', []) + results.get('fallen_angel', []) + results.get('kings_shadow', []):
            r['_sentiment'] = 'ğŸ“ˆ ë§¤ìˆ˜ ìš°ìœ„'
            b = r.get('size_bucket')
            if b == 'small': r['_insight'] = "ì†Œí˜•ì£¼ ìˆ˜ê¸‰ ë³€ê³¡ì  í¬ì°©"
            elif b == 'large': r['_insight'] = "ëŒ€í˜•ì£¼ ì¶”ì„¸ ëˆŒë¦¼ëª© í¬ì°©"
            else: r['_insight'] = "ì¤‘í˜•ì£¼ ë‚™í­ ê³¼ëŒ€ í¬ì°©"
            candidates.append(r)
        for r in results.get('overheat_short', []):
            r['_sentiment'] = 'ğŸ“‰ ë§¤ë„ ìš°ìœ„'
            r['_insight'] = "ë‹¨ê¸° ê³¼ì—´ê¶Œ ë„ë‹¬ (ê³ ì  ê²½ê³ )"
            candidates.append(r)
            
        candidates.sort(key=lambda x: abs(float(x.get('tv_z', 0))), reverse=True)
        final_rows = candidates[:5]
        
        if final_rows:
            log_signalist_today(ref_date, final_rows)

    except Exception as e:
        print(f"[ERROR] ì‹œê·¸ë„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    md = render_newsletter(ref_date)
    
    out_dir = PROJECT_ROOT / "iceage" / "out"
    out_dir.mkdir(parents=True, exist_ok=True)
    suffix = _get_newsletter_env_suffix()
    filename = f"Signalist_Daily_{ref_date}{suffix}.md"
    out_path = out_dir / filename
    
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(md)

    print(f"âœ… ìƒì„± ì™„ë£Œ: {out_path}")

if __name__ == "__main__":
    main()